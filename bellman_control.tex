\documentclass[main.tex]{subfiles}

% This code shows which label has been changed when latex complains
% \makeatletter
% \def\@testdef #1#2#3{%
% \def\reserved@a{#3}\expandafter \ifx \csname #1@#2\endcsname
% \reserved@a  \else
% \typeout{^^Jlabel #2 changed:^^J%
% \meaning\reserved@a^^J%
% \expandafter\meaning\csname #1@#2\endcsname^^J}%
% \@tempswatrue \fi}
% \makeatother

\begin{document}

\listoftodos

\section{The optimal control problem}\label{sec:bellman_optimal_control}
We now consider a particular retail
pricing problem.
Given some initial amount of stock of a product and a future
termination time,
the pricing problem is to
set the price
of the product dynamically, in order to maximise the revenue and minimise the cost of
unsold stock at the termination time.
In this section, we define the optimal control pricing problem,
describe the optimality conditions given by the Bellman equation and
solve it numerically for an example system.

Consider a system over discrete, equispaced time points
$t=0,1,\dots,T$, with state
$S_t$ and pricing process $\alpha$ such that the $\alpha_t$ that take values in a closed
interval $A=[a_{\mathrm{min}},a_{\mathrm{max}}]$ of the positive real
line $\mathbb R_+$.\footnote{In
  practice, policy values may have additional constraints that depend
  on the current state,
  in which case we say that $\alpha_t$ must take values
  in a set $A(S_t)$.
}
The state $S_t$ is the stock of a product at time $t$, and
$\alpha_t$ the price for the product in the time period from $t$ to
$t+1$. We model the amount of product sold over each time period according
to a forecast demand
function $q:A\to\mathbb R_+$, which is bounded, continuous and decreasing.
Exogenous influences on demand are considered as randomness in the
system, and are
modelled in a multiplicative fashion by a stochastic process
$W=(W_1,\dots,W_T)$ taking non-negative values. See, for
example,~\cite[Ch.~7]{talluri2006theory} for a discussion of
demand models and the modelling of uncertainty.

For a given pricing process $\alpha$, the
system evolves from some initial state $S_0>0$, according to the
recursion
\begin{equation}\label{eq:stock_dynamics}
  S_{t+1}^\alpha=S_t^\alpha-\min(S_t^\alpha,q(\alpha_t)W_{t+1}),\qquad t=0,\dots,T-1.
\end{equation}
The function $Q(s,a,w)=\min(s,q(a)w)$ denotes the unit sales over a
period at price $a$,
starting with stock $s$, and with exogenous influences
characterised by $w$.

The revenue accrued over period $t\to t+1$ is $\alpha_tQ(S_t^\alpha,\alpha_t,W_{t+1})$.
The cost of remaining stock at time $T$ is modelled by a cost per unit
stock $C\geq 0$.\footnote{In some situations, unsold items at time $T$
  may be sold at some ``salvage price'', in which case we could allow
  $C<0$.}
Let $\mathcal A$ denote the set of feasible processes that take values
in $A$.
Define the value of having stock $s$ at time $t\leq T$
by the \emph{value function} $v$, such that
\begin{align}\label{eq:value_function_def}
  v(t,s)&=\max_{\alpha\in\mathcal A} J(t,s,\alpha),\quad\text{where}\\
  J(t,s,\alpha)&=
                 \mathbb E_{W}\left[ \sum_{\tau=t}^{T-1}
                 \alpha_\tau Q(S_\tau^\alpha,\alpha_\tau,W_{\tau+1})
                 - CS_T^\alpha \mid S_t^\alpha = s
                 \right].
                 \label{eq:value_function_def2}
\end{align}
This leads us to the following mathematical formulation of the pricing
problem:
\begin{mydef}
  Given an initial amount of stock $S_0>0$ and a cost per unit unsold stock $C\geq
  0$, the \emph{pricing problem} is to find a pricing process $\alpha^*\in\mathcal A$ such that
  \begin{equation}
    J(0,S_0,\alpha^*) = v(0,S_0),
  \end{equation}
  that is, $\alpha^* = \argmax_{\alpha\in\mathcal A}J(0,S_0,\alpha)$.
\end{mydef}
We choose to maximise the expected profit over the period, which
assumes a risk-neutral decision-maker. However, it is still important
to understand the distribution of profits for a given pricing policy
$\alpha$. Therefore, we simulate the distribution when
we investigate the performance of algorithms in
\Cref{sec:suboptimal_approximations}. %,sec:markdown_miss_specification}.
If we assume that the random variables $W_t$ are independent, this
stochastic optimal control problem can be solved by
considering the optimality conditions that arise from the Dynamic
Programming principle, also known as the Bellman equation.

\subsection{Non-dimensionalisation of the system}
We will now consider a non-dimensionalised
representation of the system.
The unit of time is already scaled such that
the difference between two time points is $1$.
The units of stock will be  scaled with
respect to
the initial stock $S_0$, and units of money will be scaled with respect to
the upper bound on
price, $a_{\mathrm{max}}$.
Let the corresponding dimensionless quantities be defined with hats.
Then we set
\begin{align}
  \hat s
  &= \frac{s}{S_0},
  &\hat a
  &=\frac{a}{a_{\mathrm{max}}},
  &\hat C&=\frac{C}{a_{\mathrm{max}}},
  &\hat t &= t,
  &\hat W_{\hat t}&=W_{t}.
\end{align}
To be precise, the dimensional time $t$ should be divided by $1$ unit of time.
The dimensionless functions $\hat q(\hat a)$ and $\hat Q(\hat s,\hat
a, \hat w)$,
for forecasted demand
and realised sales respectively, are
\begin{align}
  \hat q(\hat a)&= \frac{q(\hat a\cdot a_{\max})}{S_0},
  &\hat Q(\hat s,\hat a, \hat w)&= \min(\hat s, \hat q(\hat a)\hat w).
\end{align}
The collection of pricing policies $\hat{\mathcal A}$ contains all
the processes $\alpha_t/a_{\mathrm{max}}$, where $\alpha\in \mathcal
A$. Now we can define the dimensionless value function
\begin{align}\label{eq:value_function_def_nondim}
  \hat v(\hat t,\hat s)&=\max_{\hat \alpha\in\hat{\mathcal A}}
                         \hat J(\hat t,\hat s,\hat \alpha),\quad\text{where}\\
  \hat J(\hat t,\hat s,\hat \alpha)&=
                                     \mathbb E_{\hat W}\left[ \sum_{\tau=\hat t}^{\hat T-1}
                                     \hat \alpha_\tau\hat Q(\hat
                                     {S_\tau^\alpha},\hat{\alpha_\tau},\hat
                                     W_{\tau+1})
                                     - \hat C \hat{S}_{\hat{T}}^\alpha \big\vert \hat{S}_{\hat{t}}^\alpha =
                                     \hat s
                                     \right].
                                     \label{eq:value_function_def_nondim2}
\end{align}
Finally, the dimensionless optimal control problem
is to find $\hat \alpha^*\in\hat{\mathcal A}$, such that
$\hat J(0,1,\hat \alpha^*)=\hat v(0,1)$.
For the remainder of this article, we work with the
non-dimensionalised system, and drop the hats from the
dimensionless quantities.

\subsection{The Bellman equation}
We choose $\mathcal A$ to be the set of Markovian policies for the
problem. This means that for each $\alpha\in \mathcal
A$, there exists a
measurable function $a:[0,T)\times \mathbb R_+\to A$, such that for each possible outcome
$\omega$, the process $\alpha$ is given by
$\alpha_t(\omega) =
a(t,S_t^\alpha(\omega))$.
Then,
an approach to finding the value function above is to use the Dynamic
Programming principle, which states that $v$ can be defined
recursively by
\begin{equation}\label{eq:dynamic_programming_discrete}
  v(t,s)=\max_{a\in A}\mathbb E_{W}\left[
    aQ(s,a,W_{t+1})
    +v(t+1,s-Q(s,a,W_{t+1}))\right].
\end{equation}
Thus, the value function is the solution to the backwards-in-time
recursive relation~\eqref{eq:dynamic_programming_discrete} with
terminal value $v(T,s)=-Cs$, and the optimal policy
function $a(t,s)$ is given by the argmax for each $(t,s)$.
The recursion~\eqref{eq:dynamic_programming_discrete} is called the
\emph{Bellman equation}, and a discussion
of its validity can be found, for example, in~\cite{bertsekas2005dynamic}.
Analytical solutions to Bellman equations are only available in very
rare cases, and thus numerical approaches are normally needed to
solve the equation.

We implement the following algorithm to solve the optimal control
problem, using the Bellman equation:\todo{Decorate with some algorithm environment?}
\begin{enumerate}
\item Create a grid of equispaced points $0=s_1<s_2<\cdots<s_K=S_0$, and arrays $v^K\in\mathbb R^{K\times(T+1)}$,
  $\alpha^K\in\mathbb R^{K\times T}$.
\item Set $I[v^K](s)=-Cs$.
\item Set $v^K[i,T]=I[v^K](s_i)$ for $i=1,\dots, K$.
\item For $t = T-1,\dots,0$:
  \begin{enumerate}
  \item Set $\displaystyle v^K[i,t]=\max_{a\in A}\mathbb E_{W_{t+1}}\left[ aQ(s_i,a,W_{t+1})
      +I[v^K](s_i-Q(s,a,W_{t+1}))\right]$\\ for $i=1,\dots,K$.
  \item Set $\alpha^K[i,t]$ to the maximiser above.
  \item Set $I[v^K](s) = \mathrm{Interpolate}(s, {(s_i)}_i,{(v^K[i,t])}_i)$.
  \end{enumerate}
\item Return $v^K,\alpha^K$.
\end{enumerate}
The expectation above is approximated using Monte-Carlo simulation, in
this article we use \num{1000} samples. We chose to use piecewise
linear interpolation for $I[v^K](s)$.


\subsection{Example system}\label{sec:bellman_example_markdown}
In order to investigate the optimal pricing of a specific system, we
choose to look at a family of demand functions of the form
$q(a)=q_1e^{-q_2a}$, where $q_1,q_2>0$. For a discussion
about their properties and usage in modelling demand, see~\cite[Ch.~7]{talluri2006theory}.
All the numerical experiments in this article use this family of exponential
demand functions, with price constraints $A=[0,1]$.
In the current section, and in the plots of \Cref{fig:markdown_bellman,fig:bellman_simulation,fig:bellman_det_policy_difference,fig:bellman_det_vals},
we consider a particular choice of $q_1,q_2$ so that the demand
function is given by $q(a)=\frac{1}{3}e^{2-3a}$.
In \Cref{fig:profit_diff_heatmaps} and \Cref{tbl:paramcomparisons} a larger
range of values for $q_1,q_2$ are considered.

We assume the exogenous disturbance process is a sequence of
shifted, independent and identically Beta-distributed random
variables with mean $1$ and variance
$\gamma^2$. That is, we define $W_t\sim \frac{1}{2}+X$, where
$X\sim \mathrm{Beta}(\mu,\nu)$,\footnote{A
  $\mathrm{Beta}(\mu,\nu)$ random variable has probability density
  function on $[0,1]$, given by
  $p(x)=\frac{x^{\mu-1}{(1-x)}^{\nu-1}}{B(\mu,\nu)}$.
  The function $B(\mu,\nu)$ is the normalising factor.
}
and $\mu=\nu=\frac{1}{8\gamma^2}-\frac{1}{2}$.
To ensure that $X$ is unimodal, we require that $\gamma^2<1/12$.

Set $\gamma = 5\times 10^{-2}$, $C=1$ and $T=3$.
The numerical solution to the Bellman equation, and the corresponding optimal
pricing policy is shown in \Cref{fig:markdown_bellman}.
\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
    \begin{tikzpicture}[scale=0.8]
      \begin{axis}[
        xlabel={$s$},
        ylabel={$v(t,s)$},
        title={Value function},
        legend cell align=left,
        legend pos=north west
        ]
        \addplot+[thick,mark=none] table[x index = 0,y index = 1,col sep=comma]
        {./data/markdown_bellman_det_val_policy.csv};
        \addlegendentry{$t=0$};
        \addplot+[thick,mark=none,dashed] table[x index = 0,y index = 2,col sep=comma]
        {./data/markdown_bellman_det_val_policy.csv};
        \addlegendentry{$t=1$};
        \addplot+[thick,mark=none,dash dot] table[x index = 0,y index = 3,col sep=comma]
        {./data/markdown_bellman_det_val_policy.csv};
        \addlegendentry{$t=2$};
      \end{axis}
    \end{tikzpicture}
    % \includegraphics[width=\textwidth]{./img/markdown_value_bellman}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \begin{tikzpicture}[scale=0.8]
      \begin{axis}[
        xlabel={$s$},
        ylabel={$a(t,s)$},
        title={Policy function},
        legend cell align=left,
        ]
        \addplot+[thick,mark=none] table[x index = 0,y index = 5,col sep=comma]
        {./data/markdown_bellman_det_val_policy.csv};
        \addlegendentry{$t=0$};
        \addplot+[thick,mark=none,dashed] table[x index = 0,y index = 6,col sep=comma]
        {./data/markdown_bellman_det_val_policy.csv};
        \addlegendentry{$t=1$};
        \addplot+[thick,mark=none,dash dot] table[x index = 0,y index = 7,col sep=comma]
        {./data/markdown_bellman_det_val_policy.csv};
        \addlegendentry{$t=2$};
      \end{axis}
    \end{tikzpicture}
    % \includegraphics[width=\textwidth]{./img/markdown_controls_bellman}
  \end{subfigure}
  \caption{The value function and corresponding
    optimal control function for the pricing problem.
    The regions where the value function moves from a linear to a
    nonlinear regime
    corresponds to when the pricing policy hits the upper bound 1.
  }\label{fig:markdown_bellman}
\end{figure}
Let us now investigate the behaviour of the optimal pricing policy
$\alpha$ and
the outcome of following this policy.
Define a random variable $P(\alpha)$, which for each realisation
represents the total profit,
\begin{equation}
  P(\alpha) = \sum_{t=0}^{T-1}\alpha_tQ(S_t^\alpha,\alpha_t,W_{t+1}) - CS_T^\alpha.
\end{equation}
By sampling from the stochastic process $W=(W_1,\dots,W_T)$, we can
estimate the random variables $\alpha_t$ and $P(\alpha)$.
The plots in \Cref{fig:bellman_simulation} show the results of
simulating the system \num{10000} times.\footnote{Preliminary
  investigations indicated that \num{1000}
  simulations is sufficient to obtain an accurate representation of
  the distributions in this article. However, we have chosen to use
  more when this is computationally convenient.
}

\begin{figure}[htbp]
  \centering
  \begin{subfigure}[t]{0.5\textwidth}
    \begin{tikzpicture}[scale=0.8]
      \begin{axis}[
        ylabel={Price},
        xlabel={\phantom{$P(\alpha)$}},% \phantom: to align the two figures
        title={$\alpha_t$, simulated values},
        boxplot/draw direction=y,
        xtick={1,2,3},
        xticklabels={$t=0$, $t=1$, $t=2$},
        % ymin=0.6,ymax=0.75
        ]
        \addplot+[boxplot] table[y index=0,col sep=comma]
        {./data/markdown_bellman_det_policies.csv};
        \addplot+[boxplot = {whisker range = 10}] table[y index=1,col sep=comma]
        {./data/markdown_bellman_det_policies.csv};
        \addplot+[boxplot = {whisker range = 10}] table[y index=2,col sep=comma]
        {./data/markdown_bellman_det_policies.csv};
      \end{axis}
    \end{tikzpicture}
  \end{subfigure}%
  \begin{subfigure}[t]{0.5\textwidth}
    \begin{tikzpicture}[scale=0.8]
      \begin{axis}[
        xlabel=Profit $P(\alpha)$,
        ylabel=Count,
        title={Realised profit},
        legend cell align=left,
        xmin=0.565, xmax=0.69,
        ymax=1400,
        ]
        \addplot[blue,hist={data=x,bins=40}] table [y index = 0, col
        sep=comma]
        {./data/markdown_bellman_det_vals.csv};
      \end{axis}
    \end{tikzpicture}
  \end{subfigure}%
  \caption{Simulations of the pricing system, started at $S_0=1$
    and controlled by the optimal policy $\alpha$ shown in
    \Cref{fig:markdown_bellman}.
    The left figure is a box plot
    that shows the median, the quartiles and the extremal prices observed
    in the simulations.
    As we see, the variance of the prices increases
    in time, reflecting the wider range of realised remaining stock at these times.
    The right hand figure is a histogram of the total profit over
    the pricing period.
  }\label{fig:bellman_simulation}
\end{figure}

\todo[inline]{Do we need some sort of ``concluding'' remarks here?}

\biblio
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% TeX-command-extra-options: "-shell-escape"
%%% End:
