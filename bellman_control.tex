\documentclass[main.tex]{subfiles}

% This code shows which label has been changed when latex complains
%\makeatletter
% \def\@testdef #1#2#3{%
%   \def\reserved@a{#3}\expandafter \ifx \csname #1@#2\endcsname
%  \reserved@a  \else
% \typeout{^^Jlabel #2 changed:^^J%
% \meaning\reserved@a^^J%
% \expandafter\meaning\csname #1@#2\endcsname^^J}%
% \@tempswatrue \fi}
%\makeatother

\begin{document}

\listoftodos

\section{The optimal control problem}
\todo[inline]{Say that we choose to be risk-neutral}
The overarching goal in the paper is to consider solutions to a retail
pricing problem.
\begin{mydef}[Pricing problem, informal]
  Given a stock of a product and a future terminal time,
  dynamically set the price
  of the product in order to maximise revenue and minimise the cost of
  unsold stock.
\end{mydef}
In this section, we define the optimal control pricing problem,
describe the optimality conditions given by the Bellman equation and
solve it numerically for an example system.

Consider a system over discrete time points $t=0,\dots,T$, with state
$S$ and policy process $\alpha$ that takes values in an
interval $A\subset\mathbb R_+$.
The state $S_t$ denotes the stock of product at time $t$, and
$\alpha_t$ the price for the product in the time period from $t$ to
$t+1$. We model the sales of the product according to a demand
function $q:A\to\mathbb R_+$ that is bounded, continuous and decreasing.
Randomness in the system to capture exogenous influences on demand, is
modelled in a multiplicative fashion by a Markovian stochastic process
$(W)_t$ taking non-negative values. See for example
\citet[Ch.~7]{talluri2006theory} for a discussion of
popular demand models and modelling of uncertainty.

For a given pricing process $\alpha$, the dynamics of
the system evolves from some initial state $S_0=s>0$, according to the
recursion
\begin{equation}\label{eq:stock_dynamics}
  S_{t+1}^\alpha=S_t^\alpha-\min(q(\alpha_t)W_{t+1},S_t^\alpha),\qquad t=0,\dots,T-1.
\end{equation}
The function $Q(s,a,w)=\min(q(a)w,s)$ denotes the unit sales per
period at price $a$,
starting with stock $s$, and exogenous influences
characterised by $w$.

The revenue accrued over period $t\to t+1$ is $\alpha_t(S_t^\alpha-S_{t+1}^\alpha)$.
A cost of remaining stock at time $T$ is modelled by a cost per unit
stock $C\geq 0$.\footnote{In some situations, unsold items at time $T$
  may be sold at some ``salvage price'', in which case we could allow
  $C<0$}
Let $\mathcal A$ denote the set of feasible processes that take values
in $A$.
Define the value of having stock $s$ at time $t\leq T$
by the \emph{value function}
\begin{align}
  v(t,s)&=\max_{\alpha\in\mathcal A} J(t,s,\alpha),\quad\text{where}\\
  J(t,s,\alpha)&=
                 \mathbb E_{W}\left[ \sum_{\tau=t}^{T-1}
                 \alpha_tQ(S_\tau^\alpha,\alpha_\tau,W_{\tau+1})
                 - CS_T^\alpha \mid S_t^\alpha = s
                 \right].
\end{align}
This leads us to the following mathematical formulation of the pricing
problem:
\begin{mydef}[Pricing problem]
  Given an initial stock $s>0$ and a cost per unit unsold stock $C\geq
  0$, find $\alpha\in\mathcal A$ such that
  \begin{equation}
    J(0,s,\alpha) = v(0,s).
  \end{equation}
\end{mydef}
This stochastic optimal control problem can be solved by
considering the optimality conditions that arise from the Dynamic
Programming principle, also known as the Bellman equation.


\subsection{The Bellman equation}
We assume $\alpha\in\mathcal A$ is Markovian,\todo{
  do I need to justify that the optimal policy is Markovian?}
meaning that
there exists a function $a$ such that $\alpha_t(\omega) =
a(t,S_t^\alpha(\omega))$ for each $\omega$ in the underlying probability space.
Then,
an approach to finding the value function above is to use the Dynamic
Programming principle, which states that $v$ can be defined
recursively in the following way:
\begin{align}\label{eq:dynamic_programming_discrete}
  v(t,s)&=\max_{a\in A}\mathbb E_{W}\left[
          aQ(s,a,W_{t+1})
          +v(t+1,s-Q(s,a,W_{t+1}))\right].
\end{align}
Thus, the value function is the solution to the backwards-in-time
recursive relation~\eqref{eq:dynamic_programming_discrete} with
terminal value $v(T,s)=-Cs$, and the optimal policy
function $a(t,s)$ is given by the argmax for each $t,s$.
The recursion is called the \emph{Bellman equation}, and a discussion
of its validity can be found for example in \citet{bertsekas2005dynamic}.

We implement the following algorithm to solve the optimal control
problem, using the Bellman equation:\todo{Decorate with some algorithm environment?}
\begin{enumerate}
\item Create grid $s_1,\dots,s_K$, and arrays $v^K\in\mathbb R^{K\times(T+1)}$,
  $\alpha^K\in\mathbb R^{K\times T}$
\item Set $Iv^K(s)=-Cs$
\item Set $v^K[i,T]=Iv^K(s_i)$ for $i=1,\dots, K$
\item For $t = T-1,\dots,0$
  \begin{enumerate}
  \item Set $\displaystyle v^K[i,t]=\max_{a\in A}\mathbb E_{W_{t+1}}\left[ aQ(s,a,W_{t+1})
      +Iv^K(s-Q(s,a,W_{t+1}))\right]$\\ for $i=1,\dots,K$.
  \item Set $\alpha^K[i,t]$ to the maximiser above
  \item Set $Iv^K(s) = Interpolate(s, {(s_i)}_i,{(v^K[i,t])}_i)$
  \end{enumerate}
\item Return $v^K,\alpha^K$
\end{enumerate}

\subsection{Example system}\label{sec:bellman_example_markdown}
For the rest of the paper, we consider a given non-dimensionalised
system, in such a way that stock levels lie in $[0,1]$ and the price
interval $A=[0,1]$.
Let the demand function be given by
\begin{equation}
  q(a)=\frac{1}{3}e^{2-3a}.
\end{equation}
We assume the exogenous disturbance process is a sequence of
i.i.d.~shifted Beta-distributed random variables with mean 1 and variance
$\gamma^2$. That is, we set $W_t\sim \frac{1}{2}+X$, where
$X\sim Beta(\alpha,\beta)$, and
$\alpha=\beta=\frac{1}{8\gamma^2}-\frac{1}{2}$.

Set $\gamma = 5\times 10^{-2}$, $C=1$ and $T=3$.
The solution to the Bellman equation, and the corresponding optimal
pricing policy is shown in \Cref{fig:markdown_bellman}.
\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
    \begin{tikzpicture}[scale=0.8]
      \begin{axis}[
        xlabel={$s$},
        ylabel={$v(t,s)$},
        title={Value function},
        legend cell align=left,
        legend pos=north west
        ]
        \addplot+[mark=none] table[x index = 0,y index = 1,col sep=comma]
        {./data/markdown_bellman_det_val_policy.csv};
        \addlegendentry{$t=0$};
        \addplot+[mark=none] table[x index = 0,y index = 2,col sep=comma]
        {./data/markdown_bellman_det_val_policy.csv};
        \addlegendentry{$t=1$};
        \addplot+[mark=none] table[x index = 0,y index = 3,col sep=comma]
        {./data/markdown_bellman_det_val_policy.csv};
        \addlegendentry{$t=2$};
      \end{axis}
    \end{tikzpicture}
    % \includegraphics[width=\textwidth]{./img/markdown_value_bellman}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \begin{tikzpicture}[scale=0.8]
      \begin{axis}[
        xlabel={$s$},
        ylabel={$a(t,s)$},
        title={Policy function},
        legend cell align=left,
        ]
        \addplot+[mark=none] table[x index = 0,y index = 5,col sep=comma]
        {./data/markdown_bellman_det_val_policy.csv};
        \addlegendentry{$t=0$};
        \addplot+[mark=none] table[x index = 0,y index = 6,col sep=comma]
        {./data/markdown_bellman_det_val_policy.csv};
        \addlegendentry{$t=1$};
        \addplot+[mark=none] table[x index = 0,y index = 7,col sep=comma]
        {./data/markdown_bellman_det_val_policy.csv};
        \addlegendentry{$t=2$};
      \end{axis}
    \end{tikzpicture}
    % \includegraphics[width=\textwidth]{./img/markdown_controls_bellman}
  \end{subfigure}
  \caption{The value function and corresponding
    optimal control function for the pricing problem.
    The kink in the value function
    correspond to when the pricing policy hits the upper bound 1.
  }\label{fig:markdown_bellman}
\end{figure}
Let us now investigate the behaviour of the optimal pricing policy
$\alpha$ and
the outcome of following this policy.
Define a random variable $P^\alpha$, which for each realisation
represents the total profit,
\begin{equation}
  P^\alpha = \sum_{t=0}^T\alpha_tQ(S_t^\alpha,\alpha_t,W_{t+1}) - CS_T^\alpha.
\end{equation}
If $a(t,s)$ is the function found when solving the Bellman equation,
then $\alpha$ is a stochastic process defined for each event $\omega$
from the underlying probability space, with
$\alpha_t(\omega)=a(t,S_t^\alpha(\omega))$.
By sampling from the stochastic process $W=(W_1,\dots,W_T)$, we can
estimate the random variables $\alpha_t$ and $P^\alpha$.
The plots in \Cref{fig:bellman_simulation} show the results of
simulating the system 1000 times.

\begin{figure}[htbp]
  \centering
  \begin{subfigure}[t]{0.5\textwidth}
    \begin{tikzpicture}[scale=0.8]
      \begin{axis}[
        ylabel={Price},
        xlabel={\phantom{$P^\alpha$}},
        title={$\alpha_t$, simulated values},
        boxplot/draw direction=y,
        xtick={1,2,3},
        xticklabels={$t=0$, $t=1$, $t=2$},
        %ymin=0.6,ymax=0.75
        ]
        \addplot+[boxplot] table[y index=0,col sep=comma]
        {./data/markdown_bellman_det_policies.csv};
        \addplot+[boxplot] table[y index=1,col sep=comma]
        {./data/markdown_bellman_det_policies.csv};
        \addplot+[boxplot] table[y index=2,col sep=comma]
        {./data/markdown_bellman_det_policies.csv};
      \end{axis}
    \end{tikzpicture}
  \end{subfigure}%
  \begin{subfigure}[t]{0.5\textwidth}
    \begin{tikzpicture}[scale=0.8]
      \begin{axis}[
        xlabel=$P^\alpha$,
        ylabel=Count,
        title={Realised profit},
        legend cell align=left
        ]
        \addplot[blue,hist={data=x,bins=40}] table [y index = 0, col
        sep=comma]
        {./data/markdown_bellman_det_vals.csv};
      \end{axis}
    \end{tikzpicture}
  \end{subfigure}%
  % \begin{subfigure}[b]{0.5\textwidth}
  %   \begin{tikzpicture}[scale=0.8]
  %     \begin{axis}[
  %       title={$\alpha_t^C$, simulated values},
  %       boxplot/draw direction=y,
  %       xtick={1,2,3},
  %       xticklabels={$t=0$, $t=1$, $t=2$},
  %       ymin=0.6,ymax=0.75
  %       ]
  %       \addplot+[boxplot] table[y index=3,col sep=comma]
  %       {./data/markdown_bellman_det_policies.csv};
  %       \addplot+[boxplot] table[y index=4,col sep=comma]
  %       {./data/markdown_bellman_det_policies.csv};
  %       \addplot+[boxplot] table[y index=5,col sep=comma]
  %       {./data/markdown_bellman_det_policies.csv};
  %     \end{axis}
  %   \end{tikzpicture}
  % \end{subfigure}%
  \caption{Simulations of the pricing system, started at $S_0=1$ % TODO: subscript _0 causes issues with label re-run warnings
    and
    controlled by the optimal policy $\alpha$ seen in
    \Cref{fig:markdown_bellman}.
    The left figure shows a box plot that represents the distribution of
    realised prices, and the right is a histogram of the profit over
    the pricing period. As we see, the variance of the prices increase
    in time, reflecting the wider range of remaining stock at these times.
  }\label{fig:bellman_simulation}
\end{figure}

\todo[inline]{Do we need some sort of ``concluding'' remarks here?}

\biblio
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% TeX-command-extra-options: "-shell-escape"
%%% End:
