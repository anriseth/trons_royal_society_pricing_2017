\documentclass[main.tex]{subfiles}

\begin{document}
\begin{abstract}
  In this article, we consider a retailer that wants to
  dynamically price a product affected by randomness in the demand, in order to
  maximise its profits.
  The pricing problem is formulated as a stochastic optimal
  control problem, where the optimal policy can be found by solving
  the associated Bellman equation.
  The aim is to investigate Approximate Dynamic Programming
  algorithms for this problem.
  For realistic retail applications, modelling the problem and solving
  it to optimality becomes impractical and intractable.
  Thus practitioners make simplifying assumptions and
  design suboptimal policies, but a thorough investigation
  of the relative performance of these policies is lacking.
  A challenge in Approximate Dynamic Programming is to better understand
  such trade-offs, to which this article contributes.
  We simulate the performance of two algorithms on a
  one-product system.
  It turns out that, for more than \emph{half of the realisations},
  the popular, but approximate, Certainty
  Equivalent Control policy
  yields larger profits than an optimal, expected value maximising
  policy.
  However, this approximate algorithm performs significantly worse in
  the \phantom{the} %to fix overfull hbox
  remaining realisations, implying a more risk-seeking attitude by the
  retailer.
  % However, this approximate algorithm is less robust to model misspecification, and thus
  % requires that the retailer spends more resources on state estimation.
\end{abstract}
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
