\documentclass[main.tex]{subfiles}
\usepgfplotslibrary{statistics}

\begin{document}

\listoftodos

\section{Suboptimal approximations}\label{sec:suboptimal_approximations}
\todo[inline]{Should parts of this introductory discussion go into the
  Introduction section?}
There are many reasons for practitioners to look for suboptimal
approximations to the stochastic control problem, rather than
computing the optimal policy functions for the whole decision period.
The modelling of a real system and decision process can become very
complicated, and  we must  create a very
large state space in order to make use of the Bellman equation.
We often have to take into account unobservable
state variables, like estimated parameters, and constraints\todo{Jeff:
you haven't mentioned constraints before}
that may
depend on history. Decision-makers in business may change their mind
about the objective over the course of the decision period, which should
be incorporated in the modelling as parameters with corresponding, estimated
probability distributions.
From a software implementation perspective, writing code that can
solve the Bellman equation efficiently can be much more complicated
than other methods.
Finally, the dimensions of the state, policy and exogenous
information spaces can quickly make a numerical solution to the Bellman
equation intractable. An explosion of dimensions can happen very
quickly, even for one-product problems. In
\citet{bertsimas2001dynamic}, the authors model a simple
one-product demand function with uncertainty in the function
parameters, and present an eight-dimensional dynamic programming
solution to the problem.
Much of the focus in the research community has therefore been on
developing tractable algorithms with comparable performance to the
optimal policies, see for example
\citet{powell2011approximate,bertsekas2012dynamic}.

An advantage of creating explicit policy functions at the start of the decision
process, is the speed at which we can make our decisions in the
future. In the classic pricing problems, the dynamics of the
underlying system do not require instant pricing decisions. Thus,
suboptimal decision rules that are created on-line, i.e.~as they are needed, are often used
instead (\citet{talluri2006theory}). Estimation of the system and optimisation of prices are
normally separated, and constraints are more easily updated at each
decision point.

In this section we will look at a class of algorithms known as
\emph{lookahead policies}, that calculate the decisions on-line.
Whenever a decision must be made, these methods simulate the future
behaviour of the system and optimise the current decision based on
these simulations. First, we consider a special case called the
\emph{Certainty Equivalent Control} policy, which uses a point
estimate of the system (\citet[Ch.~6]{bertsekas2005dynamic}). For the
example system in this article, it turns out that we can solve
this approximate problem analytically.
Then, we investigate lookahead policies that take into account the
variability of the future system trajectories, which produce
results that are more similar to the optimal Bellman policy.
Numerical comparison experiments between the different policies are
made using the example pricing problem introduced in
\Cref{sec:bellman_example_markdown}.

\subsection{The Certainty Equivalent Control policy}
A popular, tractable, algorithm for solving stochastic optimal control
problems is the Certainty Equivalent Control (CEC) policy, as it is
named in \citet{bertsekas2005dynamic}.
It also known as (deterministic) Model
Predictive Control in the engineering community.
The algorithm is particularly practical, because the optimisation
problem is reduced to a standard deterministic problem that can
be solved with existing, commercial or open-source solvers which
handle very large systems.
At each decision time, a deterministic optimization problem for the
remaining decision horizon is solved. Only the decision for the
current time step is
used, whilst the subsequent decisions are discarded.
For each decision time $t=0,\dots,T-1$, the CEC algorithm for the pricing
problem calculates the current price using the following steps:
\begin{enumerate}
\item Observe state $s$.
\item Take a best estimate $w_{t+1},\dots,w_T$ of ${(W_\tau)}_{t+1}^T$.
\item Solve the optimization problem
  \begin{equation}
    \max_{\mathbf a\in A^{T-t}}\left\{\sum_{\tau=t}^{T-1}\mathbf
      a_\tau Q(S_\tau^{\mathbf a},\mathbf
      a_\tau,w_{\tau})-CS_T^{\mathbf a}\right\},
    \quad \text{s.t.}\quad S_t^{\mathbf a}=s.
  \end{equation}
\item Set the price corresponding to a maximizer
  $\mathbf a_t\in A$ above.
\end{enumerate}

For the one-product pricing problem, we can simplify the optimization
problem and in some cases obtain analytical solutions for the policy function.
First, let us consider the expected value estimate $w_\tau=\mathbb E
[W_\tau]=1$ for $\tau=t+1,\dots T$.
Then we can rewrite the maximisation problem to find
the certainty equivalent value function,
\begin{equation}
  \widetilde{v}(t,s)=
  \max_{\mathbf a\in A^{T-t}}\left\{\sum_{\tau=t}^{T-1}(\mathbf
    a_\tau+C)\min\left(s-\sum_{r=t}^{\tau-1}q(\mathbf a_r),q(\mathbf a_\tau)\right)-Cs\right\}.
  % \quad \text{s.t.}\quad \sum_{\tau=t}^{T-1}q(\mathbf a_t)\leq s.
\end{equation}
The optimal choice here is to let $\mathbf a_\tau=a^*\in A$ for each
$\tau$, such that the same amount of stock is forecast to be sold in
each period.
Then, $a^*=a^C(t,s)$ is given by the policy function
\begin{equation}
  a^C(t,s)=\argmax_{a\in A} \left\{(a+C)\min\left(
      \frac{s}{T-t},q(a)
    \right)\right\}.
  % ,\quad\text{s.t.}\quad q(a)\leq \frac{s}{T-t}.
\end{equation}
Let $\mathcal P_A$ be the projection operator from $\mathbb R$ onto the interval $A$.
Let us consider two families of demand functions that are
popular in the literature, see e.g.~\citet[Ch.~7]{talluri2006theory}.
If $q$ is of the form $a\mapsto q_1-q_2a$, with $q_1,q_2> 0$, then
the optimal policy $\alpha_t=a^C(t,S_t^\alpha)$ for the deterministic
problem is given by the function
\begin{equation}
  a^C(t,s)=\mathcal P_A \left[ \max\left(
      \frac{q_1}{q_2}-\frac{s}{q_2(T-t)},\frac{1}{2}\left(\frac{q_1}{q_2}-C
      \right) \right) \right].
\end{equation}
In the case when $q(a)=q_1e^{-q_2a}$, the policy function is
\begin{equation}\label{eq:cec_policy}
  a^C(t,s)=\mathcal P_A\left[
    \max\left( \frac{1}{q_2}\log\left( \frac{q_1(T-t)}{s}\right),
      \frac{1}{q_2}-C  \right)\right].
\end{equation}
The numerical experiments in the article are all made
with a demand function of the family $q(a)=q_1e^{-q_2a}$,  for a
single combination of the  system
parameters. We refer to the appendix for a
comparison between the Bellman policy and CEC policy for a larger
range of parameters.

\todo[inline]{Can we get error bounds comparing $\widetilde{v}$ and
  $v$? Maybe using ideas from \citet{bertsekas2005survey}
  If so, write out explicit expression for $\widetilde{v}$}
% We know that $\widetilde{v}(T,s)=v(T,s)$, and that by
% Jensen's inequality, $v(T-1,s)\leq \widetilde{v}(T-1,s)$.
% Does this bound hold for $t<T-1$ as well? I'm struggling to prove it
% \begin{align}
%   \widetilde{v}(t,s)
%   &=
%     \begin{cases}
%       s& \frac{s}{T-t}\leq \frac{q_1}{e^{q_2}}\\
%       \frac{1}{q_2}(\log(q_1(T-t))s-s\log s)&
%       \frac{q_1}{e^{-q_2}}<\frac{s}{T-t}\leq
%       \frac{q_1e^{{(\frac{1}{q_2}-C)}^+}}{e^{-q_2}}\\
%       \frac{q_1(T-t)}{e^{-q_2}}\left( {[\frac{1}{q_2}-C]}^++C \right)
%       e^{{(\frac{1}{q_2}-C)}^+}-Cs&\text{otherwise}
%     \end{cases}
% \end{align}
% \todo[inline]{Is $\widetilde{v}$ concave in $s$? I think we need to
%   require $q_2\geq 1$ and maybe that $C$ only takes certain values}

\subsubsection{Example system}\label{sec:cec_comparison_example}
The example system in \Cref{sec:bellman_example_markdown} has a demand
function of the form $q(a)=q_1e^{-q_2a}$, where $q_1=e^2/3$ and
$q_2=3$. We can therefore compare the optimal Bellman policy with the
pricing policy that a CEC algorithm would create.
Denote the Bellman policy function by $a^B$, and consider
$a^B(t,s)-a^C(t,s)$ for each $t=0,\dots,T-1$. The plot in
\Cref{fig:bellman_det_policy_difference} shows this difference.
Both of the policy functions reach the upper bound in $A$ for small values
of $s$, but most of the time, the Bellman policy is pricing the products
lower than the CEC policy.
\begin{figure}[p]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      xlabel={$s$},
      ylabel={Function value},
      title={Policy functions},
      legend cell align=left,
      ]
      \addplot[mark=none] table[x index = 0,y expr={\thisrowno{5}},col sep=comma]
      {./data/markdown_bellman_det_val_policy.csv};
      \addlegendentry{$a^B(t,s)$};
      \addplot[mark=none,dashed] table[x index = 0,y expr={\thisrowno{8}},col sep=comma]
      {./data/markdown_bellman_det_val_policy.csv};
      \addlegendentry{$a^C(t,s)$};
      \addplot[mark=none] table[x index = 0,y expr={\thisrowno{6}},col sep=comma]
      {./data/markdown_bellman_det_val_policy.csv};
      \addplot[mark=none,dashed] table[x index = 0,y expr={\thisrowno{9}},col sep=comma]
      {./data/markdown_bellman_det_val_policy.csv};
      \addplot[mark=none] table[x index = 0,y expr={\thisrowno{7}},col sep=comma]
      {./data/markdown_bellman_det_val_policy.csv};
      \addplot[mark=none,dashed] table[x index = 0,y expr={\thisrowno{10}},col sep=comma]
      {./data/markdown_bellman_det_val_policy.csv};
      \draw (axis cs:0.45,0.55) node[anchor=north east] {$t=2$};
      \draw (axis cs:0.72,0.65) node[anchor=north east] {$t=1$};
      \draw (axis cs:0.75,0.75) node[anchor=south west] {$t=0$};
    \end{axis}
  \end{tikzpicture}

  \vspace{1em}
  \begin{tikzpicture}
    \begin{axis}[
      xlabel={$s$},
      ylabel={$[a^B(t,s)-a^C(t,s)]/a^B(t,s)$},
      title={\hspace{1em}Relative function difference},
      legend cell align=left,
      ]
      \addplot+[mark=none] table[x index = 0,y expr={1-\thisrowno{8}/\thisrowno{5}},col sep=comma]
      {./data/markdown_bellman_det_val_policy.csv};
      \addlegendentry{$t=0$};
      \addplot+[mark=none] table[x index = 0,y expr={1-\thisrowno{9}/\thisrowno{6}},col sep=comma]
      {./data/markdown_bellman_det_val_policy.csv};
      \addlegendentry{$t=1$};
      \addplot+[mark=none] table[x index = 0,y expr={1-\thisrowno{10}/\thisrowno{7}},col sep=comma]
      {./data/markdown_bellman_det_val_policy.csv};
      \addlegendentry{$t=2$};
    \end{axis}
  \end{tikzpicture}
  \caption{Comparison of the CEC and Bellman policy functions $a^C$ and
    $a^B$.
    The CEC function sets
    higher prices than the Bellman function for large $s$.
    Observe the spikes in the difference plot for $t<T-1$:
    In a small interval, $a^B(t,s)\geq a^C(t,s)$, before both functions
    reach the boundary price $a_{\mathrm{max}}=1$. This sudden, relative increase in
    the Bellman function compared to $a^C$ is due to the
    diffusion of the value function caused by the
    random variables $W_t$.
  }\label{fig:bellman_det_policy_difference}
\end{figure}

What is more important than how the policy function works, is how it
impacts the goal of the decision-process. Thus, we would like to see
how the two policy processes $\alpha^B$ and $\alpha^C$ perform.
One way to evaluate their performance is to look at
the distribution of the profits $P^{\alpha^B}$ and $P^{\alpha^C}$.
We remind the reader that
$P^\alpha =
\sum_{t=0}^{T-1}\alpha_tQ(S_t^\alpha,\alpha_t,W_{t+1})-CS_T^\alpha$.
From the opitimality of the Bellman policy function, we should have
that $\mathbb E_W[P^{\alpha^B}]\geq \mathbb E_W[P^{\alpha^C}]$.
However, marginalising a random variable with the expectation operator
loses a lot of information which can be of interest.
An approximation of the distributions of $P^{\alpha^B}, P^{\alpha^C}$
and their difference
$P^{\alpha^B}-P^{\alpha^C}$, based on $1000$ realisations of the
underlying $W$, can be seen in \Cref{fig:bellman_det_vals}.
Indeed in the experiment, the average value of following the
Bellman policy is higher than following the CEC policy.
However, from the bottom figure we see that
in more than half of the cases, the CEC policy outperforms the optimal
policy $\alpha^B$. What we can take from this experiment is that
the CEC policy induces a more risk-seeking pricing strategy than
$\alpha^B$: It results in slightly larger profits for a majority of the
realisations of $W$, but at a cost of taking a more significant
reduction in
profits in the remaining realisations.
\begin{figure}[p]
  \centering
  \begin{tikzpicture}[scale=0.8]
    \begin{axis}[
      xlabel=$P^{\alpha^B}$,
      ylabel=Count,
      title={Simulations of Bellman policy},
      xmin=0.57, xmax=0.69,
      ymax=180,
      ]
      \addplot[blue,hist={bins=30}] table [y index = 0,col sep=comma]
      {./data/markdown_bellman_det_vals.csv};
    \end{axis}
  \end{tikzpicture}%
  \begin{tikzpicture}[scale=0.8]
    \begin{axis}[
      xlabel=$P^{\alpha^C}$,
      title={Simulations of CEC policy},
      xmin=0.57, xmax=0.69,
      ymax=180,
      ]
      \addplot[blue,hist={bins=30}] table [y index = 1,col sep=comma]
      {./data/markdown_bellman_det_vals.csv};
      \draw[<-] (axis cs:0.62,30) --
      (axis cs:0.605,60) node[anchor=south] {Fatter lower tail};
    \end{axis}
  \end{tikzpicture}

  \vspace{1em}
  \begin{tikzpicture}
    \begin{axis}[
      xlabel=$P^{\alpha^B}-P^{\alpha^C}$,
      ylabel=Count,
      title={Simulations of Bellman and CEC policies},
      ]
      \addplot[red,hist={bins=30}] table [y expr = {(\thisrowno{0}-\thisrowno{1})},col sep=comma]
      {./data/markdown_bellman_det_vals.csv};
      \draw[dashed] (axis cs:0,-30) -- (axis cs:0,600);
      \draw[<-] (axis cs:0.017,100) -- (axis cs:0.015,200)
      node[anchor=south] {$\alpha^B$ best};
      \draw[<-] (axis cs:-0.003,400) -- (axis cs:0.005,400) node[anchor=west] {$\alpha^C$ best};
    \end{axis}
  \end{tikzpicture}
  \todo[inline]{Show relative difference
    $(P^{\alpha^B}-P^{\alpha^C})/P^{\alpha^B}$ instead?}
  \caption{This shows the distributions from $1000$ samples of
    the profits of following the Bellman and CEC policies.
    The sample mean $\mathbb E_W[P^{\alpha^B}-P^{\alpha^C}]\approx 3.8\times
    10^{-3}$, confirms that Bellman is better on average, as it should be.
    Importantly, however, in more than half the samples the suboptimal policy
    outperforms the Bellman policy.
  }\label{fig:bellman_det_vals}
\end{figure}

\biblio
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% TeX-command-extra-options: "-shell-escape"
%%% End:
