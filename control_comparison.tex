\documentclass[main.tex]{subfiles}
\usepgfplotslibrary{statistics}

\begin{document}

\listoftodos

\section{Suboptimal approximations}\label{sec:suboptimal_approximations}
\todo[inline]{Should parts of this introductory discussion go into the
  Introduction section?}
There are many reasons for practitioners to look for suboptimal
approximations to the stochastic control problem, rather than
computing the optimal policy functions for the whole decision period.
The modelling of a real system and decision process can become very
complicated, and  we must  create a very
large state space in order to make use of the Bellman equation.
We often have to take into account unobservable
state variables, like estimated parameters, and constraints that may
depend on history. Decision-makers in business may change their mind
about the objective over the course of the decision period, which should
be incorporated in the modelling as parameters with some estimated
probability distribution.
From a software implementation perspective, writing code that can
solve the Bellman equation efficiently can be much more complicated
than other methods.
Finally, the dimensions of the state, policy and exogenous
information spaces can quickly make a numerical solution to the Bellman
equation intractable. An explosion of dimensions can happen very
quickly, even for one product problems. In
\citep{bertsimas2001dynamic}, the authors model a simple
one-product demand function with uncertainty in the function
parameters, and present an eight-dimensional dynamic programming
solution to the problem.
Much of the focus in the research community has therefore been in
developing tractable algorithms with comparable performance to the
optimal policies, see for example
\citep{powell2011approximate,bertsekas2012dynamic}.

An advantage of creating explicit policy functions at the start of the decision
process, is the speed at which we can update our decisions in the
future. In the classic pricing problems, the dynamics of the
underlying system do not require instant pricing decisions. Thus,
suboptimal decision rules that are created on-line are often used
instead \citep{talluri2006theory}. Estimation of the system and optimisation of prices are
normally separated, and constraints are more easily updated at each
decision point.

In this section we will look at a class of algorithms known as
\emph{lookahead policies}, that calculate the decisions on-line.
Whenever a decision must be made, these methods simulate the future
behaviour of the system and optimise the current decision based on
these simulations. First, we consider a special case called the
\emph{Certainty Equivalent Control} policy, which uses a point
estimate of the system. For the
example system in this paper, it turns out that we can solve
this approximation analytically.
Then, we investigate lookahead policies that take into account the
variability of the future trajectories, which produce
results that are more similar to the optimal Bellman policy.
Numerical comparison experiments between the different policies are
made using the example pricing problem introduced in
\Cref{sec:bellman_example_markdown}.

\subsection{The Certainty Equivalent Control policy}
A popular, tractable algorithm for solving stochastic optimal control
problems is the Certainty Equivalent Control (CEC) policy, also known as Model
Predictive Control in the engineering community.
The algorithm is particularly practical, because the optimisation
problem is reduced to a classic deterministic problem that can
be solved with existing, commercial or open-source solvers which
handle very large systems.
At each decision point, a deterministic optimization problem for the
remaining decision horizon is solved. Only the current decision is
implemented whilst, the subsequent decisions are discarded.
For each time $t=0,\dots,T-1$, the CEC algorithm for the pricing
problem calculates the current price using the following steps:
\begin{enumerate}
\item Observe state $\hat s$.
\item Create a point estimate $w_{t+1},\dots,w_T$ of ${(W)}_{t+1}^T$.
\item Solve the optimization problem
  \begin{equation}
    \max_{\mathbf a\in A^{T-t}}\left\{\sum_{\tau=t}^{T-1}\mathbf
      a_\tau Q(S_\tau^{\mathbf a},\mathbf
      a_\tau,w_{\tau})-CS_T^{\mathbf a}\right\},
    \quad \text{s.t.}\quad S_t^{\mathbf a}=\hat s.
  \end{equation}
\item Implement the price corresponding to the maximizer $\mathbf
  a_t\in A$ above.
\end{enumerate}

For the one-product pricing problem, we can simplify the optimization
problem and in some cases get analytical solutions for the policy function.
First, let us consider the point forecast
$W\equiv 1$.
Then we can rewrite the maximisation problem to find
the certainty equivalent value function,
\begin{equation}
  \widetilde{v}(t,s)=
  \max_{\mathbf a\in A^{T-t}}\left\{\sum_{\tau=t}^{T-1}(\mathbf
    a_\tau+C)\min\left(q(\mathbf a_\tau),s-\sum_{r=t}^{\tau-1}q(\mathbf a_r)\right)-Cs\right\}.
  % \quad \text{s.t.}\quad \sum_{\tau=t}^{T-1}q(\mathbf a_t)\leq s.
\end{equation}
The optimal choice here is to let $\mathbf a_\tau=a^*\in A$ for each
$\tau$, such that the same amount of stock is sold in each period.
Then, $a^*=a^C(t,s)$ is given by the policy function
\begin{equation}
  a^C(t,s)=\argmax_{a\in A} \left\{(a+C)\min\left(
      q(a),\frac{s}{T-t}
    \right)\right\}
  % ,\quad\text{s.t.}\quad q(a)\leq \frac{s}{T-t}.
\end{equation}
Let $\mathcal P_A$ be the projection operator onto the interval $A$.
Let us consider two families of demand functions that are
popular in the literature, see e.g.~\citep[Ch.~7]{talluri2006theory}.
If $q$ is of the form $a\mapsto q_1-q_2a$, with $q_1,q_2> 0$, then
the optimal policy $\alpha_t=a^C(t,S_t^\alpha)$ for the deterministic
problem is given by the function
\begin{equation}
  a^C(t,s)=\mathcal P_A \left[ \max\left(
      \frac{q_1}{q_2}-\frac{s}{q_2(T-t)},\frac{1}{2}\left(\frac{q_1}{q_2}-C
      \right) \right) \right].
\end{equation}
In the case when $q(a)=q_1e^{-q_2a}$, the policy function is
\begin{equation}\label{eq:cec_policy}
  a^C(t,s)=\mathcal P_A\left[
    \max\left( \frac{1}{q_2}\log\left( \frac{q_1(T-t)}{s}\right),
      \frac{1}{q_2}-C  \right)\right].
\end{equation}
The numerical experiments in the paper are all made
with $q(a)=q_1e^{-q_2a}$, and for a single combination of the other system
parameters. We refer to \Cref{sec:parameter_comparison} for a
comparison between $a^B$ and $a^C$ for a larger range of parameters.

\todo[inline]{Can we get error bounds comparing $\widetilde{v}$ and
  $v$? Maybe using ideas from \citet{bertsekas2005survey}
  If so, write out explicit expression for $\widetilde{v}$}
% We know that $\widetilde{v}(T,s)=v(T,s)$, and that by
% Jensen's inequality, $v(T-1,s)\leq \widetilde{v}(T-1,s)$.
% Does this bound hold for $t<T-1$ as well? I'm struggling to prove it
% \begin{align}
%   \widetilde{v}(t,s)
%   &=
%     \begin{cases}
%       s& \frac{s}{T-t}\leq \frac{q_1}{e^{q_2}}\\
%       \frac{1}{q_2}(\log(q_1(T-t))s-s\log s)&
%       \frac{q_1}{e^{-q_2}}<\frac{s}{T-t}\leq
%       \frac{q_1e^{{(\frac{1}{q_2}-C)}^+}}{e^{-q_2}}\\
%       \frac{q_1(T-t)}{e^{-q_2}}\left( {[\frac{1}{q_2}-C]}^++C \right)
%       e^{{(\frac{1}{q_2}-C)}^+}-Cs&\text{otherwise}
%     \end{cases}
% \end{align}
% \todo[inline]{Is $\widetilde{v}$ concave in $s$? I think we need to
%   require $q_2\geq 1$ and maybe that $C$ only takes certain values}

\subsubsection{Example system}\label{sec:cec_comparison_example}
The example system in \Cref{sec:bellman_example_markdown} has a demand
function of the form $q(a)=q_1e^{-q_2a}$, where $q_1=e^2/3$ and
$q_2=3$. We can therefore compare the optimal Bellman policy with the
pricing policy that a CEC algorithm would implement.
Denote the Bellman policy function by $a^B$, and consider
$a^B(t,s)-a^C(t,s)$ for each $t=0,\dots,T-1$. The plot in
\Cref{fig:bellman_det_policy_difference} shows this difference.
Both of the policy functions reach the upper bound in $A$ for small values
of $s$, but in general the Bellman policy is pricing the products
lower than the CEC policy would do.
\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      xlabel={$s$},
      ylabel={$a^B(t,s)-a^C(t,s)$},
      title={Policy function difference},
      legend cell align=left,
      ]
      \addplot+[mark=none] table[x index = 0,y expr=\thisrowno{5}-\thisrowno{8},col sep=comma]
      {./data/markdown_bellman_det_val_policy.csv};
      \addlegendentry{$t=0$};
      \addplot+[mark=none] table[x index = 0,y expr=\thisrowno{6}-\thisrowno{9},col sep=comma]
      {./data/markdown_bellman_det_val_policy.csv};
      \addlegendentry{$t=1$};
      \addplot+[mark=none] table[x index = 0,y expr=\thisrowno{7}-\thisrowno{10},col sep=comma]
      {./data/markdown_bellman_det_val_policy.csv};
      \addlegendentry{$t=2$};
    \end{axis}
  \end{tikzpicture}
  \caption{Comparison of the CEC policy $a^C$ and
    the optimal Bellman policy $a^B$, compare with
    \Cref{fig:markdown_bellman}.
    We see that the $a^C$ sets
    higher prices than the Bellman policy function for large $s$,
    although $a^B(0,s)\geq a^C(0,s)$ in a small region before they
    both reach the
    boundary price $a_{max}=1$.
  }\label{fig:bellman_det_policy_difference}
\end{figure}

What is more important than how the policy function works, is how it
impacts the goal of the decision-process. Thus, we would like to see
how the two policy processes $\alpha^B$ and $\alpha^C$ perform.
One way to evaluate their performance is to look at
the distribution of the profits $P^{\alpha^B}$ and $P^{\alpha^C}$.
We remind the reader that
$P^\alpha =
\sum_{t=0}^{T-1}\alpha_tQ(S_t^\alpha,\alpha_t,W_{t+1})-CS_T^\alpha$.
From the definition of the Bellman policy function, we should have
that $\mathbb E_W[P^{\alpha^B}]\geq \mathbb E_W[P^{\alpha^C}]$.
However, marginalising a random variable with the expectation operator
loses a lot of information which can be of value.
An approximation of the distribution of
$P^{\alpha^B}-P^{\alpha^C}$ based on 1000 realisations of the
underlying $W$ can be seen in \Cref{fig:bellman_det_vals}.
Indeed the experiment shows that the expected value of following the
Bellman policy is better, but from the histogram we see that
in more than half of the cases, the CEC policy outperforms the optimal
policy $\alpha^B$.
\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      xlabel=$P^{\alpha^B}-P^{\alpha^C}$,
      ylabel=Count,
      title={Simulations of Bellman and CEC policies},
      ]
      % \addplot[blue,hist={bins=30}] table [y expr =
      % {(\thisrowno{0}-\thisrowno{1})/\thisrowno{0}},col sep=comma]
      \addplot[blue,hist={bins=30}] table [y expr = {(\thisrowno{0}-\thisrowno{1})},col sep=comma]
      {./data/markdown_bellman_det_vals.csv};
      \draw[dashed] (axis cs:0,-30) -- (axis cs:0,600);
      \draw[<-] (axis cs:0.017,100) -- (axis cs:0.015,200)
      node[anchor=south] {$\alpha^B$ best};
      \draw[<-] (axis cs:-0.003,400) -- (axis cs:0.005,400) node[anchor=west] {$\alpha^C$ best};
    \end{axis}
  \end{tikzpicture}
  \todo[inline]{Show relative difference
    $(P^{\alpha^B}-P^{\alpha^C})/P^{\alpha^B}$ instead?}
  \caption{This shows the distribution from 1000 samples of
    the profit of following the Bellman and CEC policy. Bars with
    positive value represent samples where Bellman performed best.
    The mean $\mathbb E_W[P^{\alpha^B}-P^{\alpha^C}]\approx 3.8\times
    10^{-3}$ confirms that Bellman is better on average, as expected.
    Importantly, in more than half the samples the suboptimal policy
    outperforms the Bellman policy.
  }\label{fig:bellman_det_vals}
\end{figure}

\subsection{Lookahead policies}
The Certainty Equivalent Control policy is a special case of
a set of rolling horizon, or lookahead policies, as defined in
\citep[Ch.~6]{powell2011approximate}.
In a general setting, these methods
calculate (sub-)optimal policies over a given \emph{control
  horizon} whilst forecasting the system behaviour over a given
\emph{prediction horizon}.
These policies are then implemented, and before the system reaches the
end of the control horizon, the process is repeated.

For the current work, all decisions will be made \emph{on-line}. This means
that the rolling horizon process is carried out at each time step, and
only the decision calculated for that time-step will be implemented to
the system. The control and prediction horizons will both be equal to
the remaining time of the control problem, so at each time $t$, the
control horizon will be $t,\dots,T-1$ and the prediction horizon
$t+1,\dots,T$.
In our implementation, we make two approximations.
First, that the expectation operator is replaced by
an average over $N_t$ samples from $(W_{t+1},\dots,W_T)$.
Second, that
the policy function can ``see into the future'', meaning that
the value at time $\tau$ is calculated based on
the current state \emph{and}  on realised values of $(W_{\tau+1},\dots W_{T})$.
Thus, at time $t$, the pricing algorithm is as follows:
\begin{enumerate}
\item Observe state $\hat s$.
\item Pick or draw $N_t$ samples $W^{(i)}$, $i=1,\dots,N_t$ from the
  underlying probability space.
\item Solve the optimization problem
  \begin{dmath}
    \max_{\substack{a_0\in A\\a_{t+1:T-1}^{(i)}\in A^{T-{t+1}}}}
    \frac{1}{N_t}\sum_{i=1}^{N_t}\left[
      a_0Q(\hat s,a_0,W_{t+1}^{(i)})
      +\sum_{\tau=t+1}^{T-1}a_s^{(i)}Q(S_\tau^{(i)},a_\tau^{(i)},W_{\tau+1}^{(i)})
      -C S_T^{(i)}\right].
  \end{dmath}
  \begin{itemize}
  \item $S^{(i)}$ corresponds to the
    controlled process in scenario $i$, starting at $S_t^{(i)}=\hat s$.
  \end{itemize}
\item Implement the controller corresponding to the maximiser $a_0\in
  A$ above.
\end{enumerate}
\textbf{Remark:} The decision variables $a_{t+1:T-1}^{(i)}$ are meant to
approximate the value the optimal policy $\alpha$ would have taken
if $W^{(i)}$ is drawn from the underlying probability space.

Note that if $N_t=1$, then this lookahead policy is the same as the CEC policy.
The number of decision variables increases linearly with $N_t$, and
so does the cost of calculating the objective.

\subsubsection{Example system}
To investigate the performance of the suboptimal lookahead
policies, we apply it to the system described in
\Cref{sec:bellman_example_markdown} and compare it to the performance
of the optimal Bellman policy in the same way as we did to the CEC
policy. We draw the same number of samples $N_t=N$ from $W$ for each
decision time $t$. Denote the resulting policy process
by $\alpha^{L_N}$. Numerical results comparing $P^{\alpha^{L_N}}$ with the
Bellman policy are shown in \Cref{fig:markdown_bellman_mpc_5_20_100}
for $N=5,20,100$. The simulations are run 1000 times each to get
approximate distributions for the $P^\alpha$s.
As $N$ increases, the behaviour of the lookahead policies become more
similar to the Bellman policy, and as a result the spread of differences in the
profits $P^\alpha$ decreases.
\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
    \begin{tikzpicture}%[trim axis left]
      \begin{axis}[
        width=\textwidth,
        xlabel=$P^{\alpha^B}-P^{\alpha^{L_N}}$,
        ylabel=Count,
        title={Simulations of Bellman and lookahead policies},
        xmin=-0.02,
        xmax=0.045,
        ymax=450,
        ]
        \addplot[blue,hist={bins=40}] table [y expr=\thisrowno{0}-\thisrowno{3},col sep=comma]
        {./data/markdown_bellman_mpc_5_20_100.csv};
        \addlegendentry{$N=5$};
      \end{axis}
    \end{tikzpicture}
  \end{subfigure}\\[1em]
  \begin{subfigure}[b]{0.5\textwidth}
    \begin{tikzpicture}%[trim axis left]
      \begin{axis}[
        width=\textwidth,
        xlabel=$P^{\alpha^B}-P^{\alpha^{L_N}}$,
        ylabel=Count,
        xmin=-0.02,
        xmax=0.045,
        ymax=450,
        ]
        \addplot[blue,hist={bins=40}] table [y expr=\thisrowno{1}-\thisrowno{4},col sep=comma]
        {./data/markdown_bellman_mpc_5_20_100.csv};
        \addlegendentry{$N=20$};
      \end{axis}
    \end{tikzpicture}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \begin{tikzpicture}
      \begin{axis}[
        width=\textwidth,
        xlabel=$P^{\alpha^B}-P^{\alpha^{L_N}}$,
        xmin=-0.02,
        xmax=0.045,
        ymax=450,
        ]
        \addplot[blue,hist={bins=40}] table [y expr=\thisrowno{2}-\thisrowno{5},col sep=comma]
        {./data/markdown_bellman_mpc_5_20_100.csv};
        \addlegendentry{$N=100$};
      \end{axis}
    \end{tikzpicture}
  \end{subfigure}
  \caption{The difference in value using a Bellman policy and a
    lookahead policy with $N$ future scenarios.
    The histograms show the result from 1000 individual
    simulations of the system.
    As $N$ increases, the differences between the lookahead policy and
    the optimal Bellman controller decreases. For each $N$, the Bellman
    policy achieves a higher mean outcome over the 1000 simulations.
  }\label{fig:markdown_bellman_mpc_5_20_100}
\end{figure}

\biblio
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% TeX-command-extra-options: "-shell-escape"
%%% End:
