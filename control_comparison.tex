\documentclass[main.tex]{subfiles}
\usepgfplotslibrary{statistics}

\begin{document}
\pgfmathdeclarefunction{gauss}{3}{%
  \pgfmathparse{1/(#3*sqrt(2*pi))*exp(-((#1-#2)^2)/(2*#3^2))}%
}

\pgfmathdeclarefunction{gamma}{1}{%
  \pgfmathparse{2.506628274631*sqrt(1/#1)+ 0.20888568*(1/#1)^(1.5)+
    0.00870357*(1/#1)^(2.5)- (174.2106599*(1/#1)^(3.5))/25920-
    (715.6423511*(1/#1)^(4.5))/1244160)*exp((-ln(1/#1)-1)*#1)}%
}

\pgfmathdeclarefunction{gammapdf}{3}{%
  \pgfmathparse{1/(#3^#2)*1/(gamma(#2))*#1^(#2-1)*exp(-#1/#3)}%
}

\listoftodos

\section{Suboptimal approximations}
There are many reasons for practitioners to look for suboptimal
approximations to the stochastic control problem, rather than
computing the optimal policy functions for the whole decision period.
The modelling of a real system and decision process can become very
complicated, and  we must one must create a very
large state space in order to make use of the Bellman equation.
We often have to take into account unobservable
state variables, like estimated parameters, and constraints that may
depend on history. Decision-makers in business may change their mind
about the objective over the course of the decision period, which should
be incorporated in the modelling as parameters with some estimated
probability distribution.
From a software implementation perspective, writing code that can
solve the Bellman equation efficiently can be much more complicated
than other methods.
Finally, the dimensions of the state space, policy space and exogenous
information space can quickly make a numerical solution to the Bellman
equation intractable. An explosion of dimensions can happen very
quickly, even for one product problems. In
\citep{bertsimas2001dynamic}, the authors model a very simple
one-product demand function with uncertainty in the function
parameters, and present an eight-dimensional dynamic programming
solution to the problem.
Much of the focus in the research community has therefore been in
developing tractable algorithms with comparable performance to the
optimal policies, see for example
\citep{powell2011approximate,bertsekas2012dynamic}.

An advantage of creating explicit policy functions at the start of the decision
process, is the speed at which we can update our decisions in the
future. In the classic pricing problems, the dynamics of the
underlying system do not require instant pricing decisions. Thus,
suboptimal decision rules that are created on-line are often used
instead \citep{talluri2006theory}. Estimation of the system and optimisation of prices are
normally separated, and constraints are more easily updated at each
decision point.

In this section we will look at a class of algorithms known as
\emph{lookahead policies}, that calculate the decisions on-line.
Whenever a decision must be made, these methods simulate the future
behaviour of the system and optimise the current decision based on
these simulations. First, we consider the \emph{certainty equivalent
  controller}, which uses a point estimate of the system. For the
example system in this paper, it turns out that we can solve
this approximation analytically.
Then, we look at lookahead policies that take into account the
variability of the future trajectories, and see that these produce
results that are more similar to the optimal Bellman controller.

\subsection{The Certainty Equivalent Controller}
A popular, tractable algorithm for solving stochastic optimal control
problems is the Certainty Equivalent Controller(CEC), also known as Model
Predictive Control in the engineering community.
The algorithm is particularly practical, because the optimisation
problem is reduced to a classic deterministic problem that can
be sent to existing, commercial or open-source solvers and that can
solve very large systems.
At each decision-point, a deterministic optimization problem for the
remaining decision-horizon is solved, but only the current decision is
implemented whilst the rest are discarded.
For each time $t=0,\dots,T-1$, the CEC algorithm for the pricing
problem calculates the current price using the following steps:
\begin{enumerate}
\item Observe state $\hat s$.
\item Create a point estimate $w_{t+1},\dots,w_T$ of ${(W)}_{t+1}^T$.
\item Carry out the optimization.
  \begin{equation}
    \max_{\mathbf a\in A^{T-t}}\left\{\sum_{\tau=t}^{T-1}\mathbf
      a_\tau Q(S_\tau^{\mathbf a},\mathbf
      a_\tau,w_{\tau})-CS_T^{\mathbf a}\right\},
    \quad \text{s.t.}\quad S_t^{\mathbf a}=\hat s.
  \end{equation}
\item Implement the price corresponding to the maximizer $\mathbf
  a_t\in A$ above.
\end{enumerate}

\todo[inline]{First write the general update on-line formula, then
  say that we can find a closed form solution}

First, let us assume the system is deterministic, with
$W\equiv 1$.
Then the pricing problem is a classic finite-dimensional optimisation
problem
\begin{equation}
  v(t,s)=\max_{\mathbf a\in A^{T-t}}\left\{\sum_{\tau=t}^{T-1}(\mathbf
    a_\tau+C)q(\mathbf a_\tau)-Cs\right\},
  \quad \text{s.t.}\quad \sum_{\tau=t}^{T-1}q(\mathbf a_t)\leq s.
\end{equation}
The optimal choice here is to choose $\mathbf a_t=a^*\in A$ for each
$t$, where $a^*$ solves
\begin{equation}
  \max_{a\in A} \left\{(a+C)q(a)\right\},\quad\text{s.t.}\quad
  q(a)\leq \frac{s}{t}.
\end{equation}

We solve the problem for two families of demand functions popular in
the literature, see e.g.~\citep[Ch.~7]{talluri2006theory}.
Let $\mathcal P_A$ be the projection operator onto the interval $A$.
If $q$ is of the form $a\mapsto q_1-q_2a$, with $q_1,q_2> 0$, then
the optimal policy $\alpha_t=a(t,S_t^\alpha)$ for the deterministic
problem is given by the function
\begin{equation}
  a(t,s)=\mathcal P_A \left[ \max\left(
      \frac{q_1}{q_2}-\frac{s}{q_2(T-t)},\frac{1}{2}\left(\frac{q_1}{q_2}-C
      \right) \right) \right].
\end{equation}
In the case when $q(a)=q_1e^{-q_2a}$, the policy function is
\begin{equation}
  a(t,s)=\mathcal P_A\left[
    \max\left( \frac{1}{q_2}\log\left( \frac{T-t}{q_1s}\right),
      \frac{1}{q_2}-C  \right)\right].
\end{equation}

\begin{figure}[htbp]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
      xlabel={$s$},
      ylabel={$a^B(t,s)-a^C(t,s)$},
      title={Policy function difference},
      legend cell align=left,
      ]
      \addplot+[mark=none] table[x index = 0,y expr=\thisrowno{5}-\thisrowno{8},col sep=comma]
      {./data/markdown_bellman_det_val_policy.csv};
      \addlegendentry{$t=0$};
      \addplot+[mark=none] table[x index = 0,y expr=\thisrowno{6}-\thisrowno{9},col sep=comma]
      {./data/markdown_bellman_det_val_policy.csv};
      \addlegendentry{$t=1$};
      \addplot+[mark=none] table[x index = 0,y expr=\thisrowno{7}-\thisrowno{10},col sep=comma]
      {./data/markdown_bellman_det_val_policy.csv};
      \addlegendentry{$t=2$};
    \end{axis}
  \end{tikzpicture}
  \caption{Comparison of the certainty equivalent controller $a^C$ and
    the optimal Bellman controller $a^B$, compare with
    \Cref{fig:markdown_bellman}.
    We see that the $a^C$ sets
    higher prices than the Bellman controller, until they both reach the
    boundary price $a_{max}=1$.
  }\label{fig:bellman_det_policy_difference}
  \todo[inline]{Reference this}
  \todo[inline]{Move this to comparison place?}
\end{figure}

% \subsection{Lookahead policies}
% We will now describe a suboptimal approach to the optimal control
% problem, popular for control problems where the frequency of decisions
% is not too high. The approach is often referred to as \emph{Rolling
%   Horizon Policies}, and is a type of
% \emph{Lookahead Policy} as defined in \citet[Ch.~6]{powell2011approximate}.
% When calculating the optimal policy for the entire control
% period $t=0,\dots,T$, one can calculate (sub-)optimal policies over a given \emph{control
%   horizon} whilst forecasting the system behaviour over a given
% \emph{prediction horizon}.
% These policies are then implemented, and before the system reaches the
% end of the control horizon, the process is repeated.

% For the current work, all decisions will be made \emph{on-line}. This means
% that the rolling horizon process is carried out at each time step, and
% only the control calculated for that time-step will be implemented to
% the system. The control and prediction horizons will both be equal to
% the remaining time of the control problem, so at each time $t$, the
% control horizon will be $t,\dots,T-1$ and the prediction horizon
% $t,\dots,T$.

% The optimal rolling horizon strategy will be to, for each time step
% $t$,
% \begin{enumerate}
% \item Observe state $\hat x$ at time $t$
% \item Calculate $v(t,\hat x)$, with the maximising control
%   $\hat \alpha\in\mathcal A(t,\hat x)$
% \item Implement the control $\hat\alpha_t$
% \end{enumerate}
% This approach will yield the same optimal control process
% as the global maximiser $\alpha^*\in\mathcal A(0,X_0)$ of
% $v(0,X_0)$ in \Cref{eq:value_function_discrete}.\footnote{Provided
%   that the true underlying probability measure is dominated by our
%   choice of probability measure. This ensures that any observed state
%   $\hat x$ has been ``considered'' by $\alpha^*$.}
% For any practical application, the rolling horizon strategy will
% compute sub-optimal policies from subsets of $\mathcal A(t,\hat x)$,
% based on approximations of the objective function in $v(t,\hat x)$.
% In this document we consider open-loop controllers, and approximate
% the expectation operator by considering one or more scenarios drawn from
% the underlying probability space.
% Thus, at time $t$, the controller carries out the following:
% \begin{enumerate}
% \item Observe state $\hat x$
% \item Pick or draw $N_t$ scenarios $\omega^{(i)}$, $i=1,\dots,N_t$ from the
%   underlying probability space.
% \item Carry out the optimization
%   \begin{align}
%     \max_{\substack{a\in A(t,\hat x)\\a_{t+1:T-1}^{(i)}}}
%     \frac{1}{N_t}\sum_{i=1}^{N_t}\left[ U(t+1,\hat x, a,\omega_{t+1}^{(i)})
%     +\sum_{s=t+1}^{T-1}U(s+1,X_s^{(i)}, a_s^{(i)},\omega_{s+1}^{(i)})
%     + \overline{U}(X_T^{(i)})\right]
%   \end{align}
%   \begin{itemize}
%   \item $X^{(i)}$ corresponds to the
%     controlled process in scenario $i$, starting at $X_t^{(i)}=\hat x$.
%   \item The decision variables $a_{t+1:T-1}^{(i)}$ are constrained by
%     the implicitly defined sets
%     $A(s,X_s^{(i)})$
%   \end{itemize}
% \item Implement the controller corresponding to the maximiser $a\in
%   A(t,\hat x)$ above.
% \end{enumerate}
% This algorithm turns the infinite-dimensional optimisation problem
% in \Cref{eq:value_function_discrete} into a sequence of
% finite-dimensional optimisation problems that can be solved with any
% existing appropriate solver. Note that the control values $a^{(i)}$
% know the future trajectory of the system, so they are cheating by
% looking into the future.
% Also note that the number of decision
% variables increases linearly in $N_t$.
% If $N_t=1$, we call this Lookahead policy \emph{Certainty Equivalent
%   Control}.
% It also shares many of the features of \emph{Model Predictive
%   Control}, popular in the process engineering industry.

% \subsection{Example: Rolling horizon performance}
% To investigate the performance of the suboptimal rolling horizon
% controller, we apply it to the system described in
% \Cref{sec:bellman_example_markdown} and compare it to the performance
% of the optimal Bellman controller. The initial state is set to $X_0=1$.
% We draw 1000 samples from the underlying probability distribution of
% $\omega$, apply the control strategies on the system, and calculate
% the total value as described by the objective function for each trajectory.
% For each simulation $i=1,\dots,1000$, the value of the
% resulting Bellman and Rolling horizon controllers are denoted by
% $V_B^{(i)}$ and $V_L^{(i)}$ respectively.
% % A distribution  of the $V_B^{(i)}$ is shown in
% % \Cref{fig:markdown_bellman_values}. The mean of the $V_B^{(i)}$s
% % is within $10^{-4}$ of the approximated value function
% % $v(0,X_0)$, calculated using the Bellman equation~\eqref{eq:dynamic_programming_discrete}.
% % \begin{figure}[htbp]
% %   \centering
% %   \begin{tikzpicture}
% %     \begin{axis}[
% %       xlabel=$V_B$,
% %       ylabel=Count,
% %       title={Simulations of Bellman controlled trajectories},
% %       legend cell align=left
% %       ]
% %       \addplot[blue,hist={data=x,bins=40}] file {./data/markdown_bellman_values.dat};
% %     \end{axis}
% %   \end{tikzpicture}
% %   \caption{Histogram showing the spread of $V_B$ from 1000 simulated trajectories using
% %     the optimal Bellman controller.}\label{fig:markdown_bellman_values}
% % \end{figure}
% We expect the mean of the $V_B^{(i)}$ to be higher than $V_L^{(i)}$,
% as the Bellman controllers are optimal. This may not be true for each
% realisation of $\omega$, and from the experiments we observe that this
% is indeed correct. In \Cref{fig:markdown_bellman_mpc_1_20_100}, a
% fitted
% distribution to the 1000 outcomes $V_B^{(i)}-V_L^{(i)}$ is plotted for
% rolling horizon controllers with $N_t=N=1,20,100$ scenarios.

% \noindent\textbf{Some remarks:}

% 1) In the case of $N=1$, the Certainty Equivalent Control (CEC), we chose
% to set $\omega^{(1)} = \mathbb
% E[\omega]$, so the on-line decision only considered the ``most
% likely'' scenario.

% 2) For the current, simple example, calculating the optimal control
% policy off-line using the Bellman equation was much faster than the
% optimizations that had to be done for rolling horizon with $N=20,100$.

% 3) The performance of the CEC
% varies a lot, and does actually outperform the Bellman control in
% fifty percent of the simulations. This comes at a higher price, as
% the under-performance is much higher in many cases. This
% ``disregard'' of the stochasticity in the system seem to have
% created a ``high-risk, high-reward'' controller.

% \begin{figure}[htbp]
%   \centering
%   \begin{subfigure}[b]{0.5\textwidth}
%     \begin{tikzpicture}[trim axis left]
%       \begin{axis}[
%         width=\textwidth,
%         xlabel=$V_B-V_L$,
%         ylabel=Count,
%         title={Simulations of Bellman and Lookahead controls},
%         xmin=-0.006,
%         xmax=0.022,
%         ymax=530
%         ]
%         \addplot[blue,hist={bins=40}] table [y index = 0,col sep=comma]
%         {./data/markdown_bellman_mpc_1_20_100.csv};
%         \addlegendentry{$N=1$};
%       \end{axis}
%     \end{tikzpicture}
%   \end{subfigure}\\[1em]
%   \begin{subfigure}[b]{0.5\textwidth}
%     \begin{tikzpicture}[trim axis left]
%       \begin{axis}[
%         width=\textwidth,
%         xlabel=$V_B-V_L$,
%         ylabel=Count,
%         xmin=-0.006,
%         xmax=0.022,
%         ymax=530
%         % title={Simulations of Bellman and Lookahead controls}
%         ]
%         \addplot[blue,hist={bins=40}] table [y index = 1,col sep=comma]
%         {./data/markdown_bellman_mpc_1_20_100.csv};
%         \addlegendentry{$N=20$};
%       \end{axis}
%     \end{tikzpicture}
%   \end{subfigure}%
%   \begin{subfigure}[b]{0.5\textwidth}
%     \begin{tikzpicture}
%       \begin{axis}[
%         width=\textwidth,
%         xlabel=$V_B-V_L$,
%         xmin=-0.006,
%         xmax=0.022,
%         ymax=530
%         % title={Simulations of Bellman and Lookahead controls}
%         ]
%         \addplot[blue,hist={bins=40}] table [y index = 2,col sep=comma]
%         {./data/markdown_bellman_mpc_1_20_100.csv};
%         \addlegendentry{$N=100$};
%       \end{axis}
%     \end{tikzpicture}
%   \end{subfigure}
%   \caption{The difference in value using a Bellman controller and a
%     lookahead control policy with $N$ future scenarios.
%     The histograms show the result from 1000 individual
%     simulations of the system.
%     As $N$ increases, the differences between the lookahead policy and
%     the optimal Bellman controller decreases. For each $N$, the Bellman
%     controller achieves a higher mean outcome over the 1000 simulations.
%   }\label{fig:markdown_bellman_mpc_1_20_100}
% \end{figure}

% \subsection{Model uncertainty, or miss-specification}\label{sec:markdown_miss_specification}
% It is difficult to correctly model the true underlying disturbance
% $\omega^\dagger$, and the performance of the controller should be
% tested to different types of model miss-specifications.
% Say the controllers are calculated based on
% a system disturbance $\omega\sim \mathcal N(1,0.05^2)$, and consider
% two cases where the true disturbance is different:
% \begin{enumerate}
% \item Mean miss-specification, $\omega^\dagger\sim \mathcal
%   N(0.95,0.05^2)$.
% \item Standard deviation miss-specification, $\omega^\dagger\sim
%   \mathcal N(1,0.1^2)$.
% \end{enumerate}
% We simulate the system for both of these cases 1000 times each,
% and check the performance of the optimal Bellman controller and the
% Certainty Equivalent Controller. The results are shown in
% \Cref{fig:markdown_bellman_mpc_1_20_100}.
% The relative performance of the CEC is drastically reduced when the
% assumed mean is wrong, i.e.\ when a poor quality forecast is taken as
% the truth.

% \begin{figure}[htbp]
%   \centering
%   \begin{subfigure}[b]{0.5\textwidth}
%     \begin{tikzpicture}[trim axis left]
%       \begin{axis}[
%         width=\textwidth,
%         legend pos=north west,
%         xlabel=$V_B$,
%         ylabel=Count,
%         title={Mean miss-specification},
%         xmin=0.52, xmax=0.71,
%         ymax=150,
%         ]
%         \addplot[blue,hist={bins=40}] table [y index = 0,col sep=comma]
%         {./data/markdown_bellman_mpc_model_misspecification.csv};
%         \addlegendentry{$\mathbb E[\omega^\dagger]=0.95$};
%       \end{axis}
%     \end{tikzpicture}
%   \end{subfigure}\hfill
%   \begin{subfigure}[b]{0.5\textwidth}
%     \begin{tikzpicture}
%       \begin{axis}[
%         width=\textwidth,
%         legend pos=north west,
%         xlabel=$V_B$,
%         title={Deviation miss-specification},
%         xmin=0.52, xmax=0.71,
%         ymax=150
%         ]
%         \addplot[blue,hist={bins=40}] table [y index = 1,col sep=comma]
%         {./data/markdown_bellman_mpc_model_misspecification.csv};
%         \addlegendentry{$\mbox{std}[\omega^\dagger]=0.1$};
%       \end{axis}
%     \end{tikzpicture}
%   \end{subfigure}\\[1em]
%   \begin{subfigure}[b]{0.5\textwidth}
%     \begin{tikzpicture}[trim axis left]
%       \begin{axis}[
%         width=\textwidth,
%         % legend style={at={(0.5,0.97)}, anchor=north},
%         xlabel=$V_B-V_L$,
%         ylabel=Count,
%         xmin=-0.007, xmax=0.025,
%         ymax=500
%         ]
%         \addplot[blue,hist={bins=40}] table [y index = 4,col sep=comma]
%         {./data/markdown_bellman_mpc_model_misspecification.csv};
%         \addlegendentry{$\mathbb E[\omega^\dagger]=0.95$};
%       \end{axis}
%     \end{tikzpicture}
%   \end{subfigure}\hfill
%   \begin{subfigure}[b]{0.5\textwidth}
%     \begin{tikzpicture}
%       \begin{axis}[
%         width=\textwidth,
%         % legend style={at={(0.5,0.97)}, anchor=north},
%         xlabel=$V_B-V_L$,
%         xmin=-0.007, xmax=0.025,
%         ymax=500
%         ]
%         \addplot[blue,hist={bins=40}] table [y index = 5,col sep=comma]
%         {./data/markdown_bellman_mpc_model_misspecification.csv};
%         \addlegendentry{$\mbox{std}[\omega^\dagger]=0.1$};
%       \end{axis}
%     \end{tikzpicture}
%   \end{subfigure}
%   \caption{Performance of the Bellman controller and Certainty
%     Equivalent Controller (N = 1) under model miss-specification of
%     $\omega$. Compare with \Cref{fig:markdown_bellman_mpc_1_20_100}.
%     The controllers are calculated under the assumption that
%     the system disturbance is $\omega\sim\mathcal N(1,0.05^2)$, instead
%     of the true disturbance~$\omega^\dagger$.
%     The CEC performance is very dependent on mean miss-specification,
%     as expected.
%   }\label{fig:markdown_bellman_mpc_model_misspecification}
% \end{figure}

% \subsubsection{Higher-order model uncertainty}
% The model approximation of the two first moments of the underlying
% disturbance is likely to be more accurate than for higher moments.
% It can therefore be interesting to see how the different control
% policies deal with model uncertainty of higher moments.
% We consider affine transformations of a $\chi^2_5$ distribution with five degrees of
% freedom, that have the \emph{same mean and standard deviation as the model
%   distribution $\omega$}. A comparison of the probability density
% functions can be seen in \Cref{fig:chisq_transformed}.

% \begin{figure}[htbp]
%   \centering
%   \begin{subfigure}[b]{0.5\textwidth}
%     \begin{tikzpicture}
%       \begin{axis}[
%         width=\textwidth,
%         no markers, domain=0.9:1.25, samples=200,
%         xmin=0.75, xmax=1.25,
%         xlabel=$w$, ylabel=Density,
%         legend cell align=left,
%         title={Positive skewness}
%         ]
%         \addplot+[thick] {gammapdf(max((x-0.921)/0.0158,
%           0),2.5,2)/0.0158};
%         \addlegendentry{$\omega_1^{\dagger}$};
%         \addlegendentry{$\omega$};
%         \addplot+[thick,dashed,domain=0.75:1.25] {gauss(x,1.0,0.05)};
%       \end{axis}
%     \end{tikzpicture}
%   \end{subfigure}%
%   \begin{subfigure}[b]{0.5\textwidth}
%     \begin{tikzpicture}
%       \begin{axis}[
%         width=\textwidth,
%         no markers, domain=0.75:1.1, samples=200,
%         xmin=0.75, xmax=1.25,
%         xlabel=$w$, ylabel=Density,
%         legend cell align=left,
%         title={Negative skewness}
%         ]
%         \addplot+[thick] {gammapdf(max((1.079-x)/0.0158,
%           0),2.5,2)/0.0158};
%         \addlegendentry{$\omega_2^{\dagger}$};
%         \addlegendentry{$\omega$};
%         \addplot+[thick,dashed,domain=0.75:1.25] {gauss(x,1.0,0.05)};
%       \end{axis}
%     \end{tikzpicture}
%   \end{subfigure}
%   \caption{Probability density comparisons between
%     the model disturbance $\omega$ and a true underlying
%     model. A larger positive tail on the left ($\omega_1^\dagger$) and
%     a larger negative tail on the right ($\omega_2^\dagger$).
%   }\label{fig:chisq_transformed}
% \end{figure}
% We again simulate the system with the Bellman and CEC controllers 1000
% times, and compare the outomes. The results are shown in
% \cref{fig:markdown_bellman_mpc_chi2}. We see that in the two cases considered,
% the relative performance of the CEC controller does not change much
% compared to the case where the assumed uncertainty $\omega$ is the same as
% the true uncertainty $\omega^\dagger$.

% \begin{figure}[htbp]
%   \centering
%   \begin{subfigure}[b]{0.5\textwidth}
%     \begin{tikzpicture}[trim axis left]
%       \begin{axis}[
%         width=\textwidth,
%         legend pos=north west,
%         xlabel=$V_B$,
%         ylabel=Count,
%         title={Skewness under-specification},
%         xmin=0.58, xmax=0.69,
%         ymax=150,
%         ]
%         \addplot[blue,hist={bins=40}] table [y index = 2,col sep=comma]
%         {./data/markdown_bellman_mpc_model_misspecification.csv};
%         \addlegendentry{$\omega_1^\dagger$};
%       \end{axis}
%     \end{tikzpicture}
%   \end{subfigure}\hfill
%   \begin{subfigure}[b]{0.5\textwidth}
%     \begin{tikzpicture}
%       \begin{axis}[
%         width=\textwidth,
%         legend pos=north west,
%         xlabel=$V_B$,
%         title={Skewness over-specification},
%         xmin=0.58, xmax=0.69,
%         ymax=150
%         ]
%         \addplot[blue,hist={bins=40}] table [y index = 3,col sep=comma]
%         {./data/markdown_bellman_mpc_model_misspecification.csv};
%         \addlegendentry{$\omega_2^\dagger$};
%       \end{axis}
%     \end{tikzpicture}
%   \end{subfigure}\\[1em]
%   \begin{subfigure}[b]{0.5\textwidth}
%     \begin{tikzpicture}[trim axis left]
%       \begin{axis}[
%         width=\textwidth,
%         % legend style={at={(0.5,0.97)}, anchor=north},
%         xlabel=$V_B-V_L$,
%         ylabel=Count,
%         xmin=-0.006, xmax=0.022,
%         ymax=600
%         ]
%         \addplot[blue,hist={bins=40}] table [y index = 6,col sep=comma]
%         {./data/markdown_bellman_mpc_model_misspecification.csv};
%         \addlegendentry{$\omega_1^\dagger$};
%       \end{axis}
%     \end{tikzpicture}
%   \end{subfigure}\hfill
%   \begin{subfigure}[b]{0.5\textwidth}
%     \begin{tikzpicture}
%       \begin{axis}[
%         width=\textwidth,
%         % legend style={at={(0.5,0.97)}, anchor=north},
%         xlabel=$V_B-V_L$,
%         xmin=-0.006, xmax=0.022,
%         ymax=600
%         ]
%         \addplot[blue,hist={bins=40}] table [y index = 7,col sep=comma]
%         {./data/markdown_bellman_mpc_model_misspecification.csv};
%         \addlegendentry{$\omega_2^\dagger$};
%       \end{axis}
%     \end{tikzpicture}
%   \end{subfigure}
%   \caption{Performance of Bellman and CEC when the true system disturbances
%     are transformations $\omega_1^\dagger,\omega_2^\dagger$ of
%     a $\chi_5^2$ distribution. Compare to
%     \Cref{fig:markdown_bellman_values,fig:markdown_bellman_mpc_1_20_100}.
%     The relative performance of CEC compared to Bellman does not
%     change much,
%     although we see a slight deterioration for the under-estimation of
%     skewness (left) and a slight improvement when the skewness has
%     been over-estimated (right).
%   }\label{fig:markdown_bellman_mpc_chi2}
% \end{figure}

% \subsection{Discussion}
% The Bellman controller is easy to calculate for small-sized problems,
% but numerical solutions to the Bellman equation spends much time
% computing the value function in states that a the given system is
% highly unlikely to end up in. That can be useful if low-probability
% events occur, although I believe it is more likely that such events
% ``surprise'' us due to model miss-specification of the underlying
% uncertainty.

% Lookahead policies with on-line computation of controllers can use
% powerful deterministic solvers, they make it easier to handle
% state-dependent constraints, and to update our beliefs for the system
% dynamics with filtering techniques.
% The lookahead policy performance was similar to the Bellman controller
% in our example, especially when we consider multiple future scenarios.
% One computational issue is that the optimisation problem carried out
% at each step increases linearly in size with number of scenarios.
% It is therefore critical to such policies that the system dynamic is slow
% enough to allow the on-line optimisation step to be calculated before
% the control period has passed.

% The Certainty Equivalent Control policy is very fast, but
% is sensitive to the forecast uncertainty. Over time, combining
% CEC with good filtering rules may result in a fast and accurate controller.
% The stability investigations of model uncertainty miss-specification
% in \Cref{sec:markdown_miss_specification} indicate that the
% relative CEC performance is fairly stable to perturbations in the second- and higher order
% moments, but less so to perturbations in the true mean.

% \todo[inline]{Jeff idea: The ``robustness'' of Bellman may be because of the
% ``stability properties'' of the parabolic equation (think in cts time).}

\biblio
\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% TeX-command-extra-options: "-shell-escape"
%%% End:
